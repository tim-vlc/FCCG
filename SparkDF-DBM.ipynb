{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1ef915-0a7f-4956-b1c2-4ffd9149d2d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum as _sum, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccad58e6-3390-4a35-b1b3-92d1197135cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_table_df(path):\n",
    "    \"\"\"\n",
    "    return df from csv file\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd3ae8d-db5a-4d2f-9c10-a7c20e1772d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = load_table_df('/FileStore/tables/web_Google.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c02279-64bb-4aee-837a-3d58a3dd92be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_table_df(df):\n",
    "    \"\"\"\n",
    "    clean the df imported\n",
    "    \"\"\"\n",
    "    col = df.columns[0]\n",
    "    return df.withColumn('key', split(df[col], '\\t').getItem(0).cast(IntegerType())) \\\n",
    "             .withColumn('value', split(df[col], '\\t').getItem(1).cast(IntegerType())) \\\n",
    "             .drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883184f1-0658-47ef-9039-325de4ca86ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = clean_table_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914aaae4-14a8-4aba-bb0d-959bf78cd97e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fccg_iterate(df):\n",
    "    # MAP\n",
    "    map_df = df.union(df.select(col(\"value\").alias(\"key\"), col(\"key\").alias(\"value\")))\n",
    "\n",
    "    # REDUCE\n",
    "    # Group by 'key' and aggregate values into a list, and find minimum value in each group\n",
    "    df_agg = map_df.groupBy(\"key\").agg(\n",
    "        collect_set(\"value\").alias(\"adj_list\"),\n",
    "        array_min(collect_set(\"value\")).alias(\"min_value\")\n",
    "        )\n",
    "\n",
    "    # Filter the rows that have min_value lower than key\n",
    "    df_agg_filtered = df_agg.filter(col(\"key\")>col(\"min_value\"))\n",
    "\n",
    "    # Calculate the new pairs created (sum of values in the adjacency list)\n",
    "    new_count = df_agg_filtered.select(_sum(size(\"adj_list\"))).first()[0] - df_agg_filtered.count()\n",
    "\n",
    "    # Concat the key column with the adj_list one into one array\n",
    "    df_concat = df_agg_filtered.select(concat(array(col(\"key\")), col(\"adj_list\")).alias(\"adj_list\"), col(\"min_value\"))\n",
    "    # Explode the 'values' list to separate rows and include 'min_value' for comparison\n",
    "    df_exploded = df_concat.select(\n",
    "        explode(\"adj_list\").alias(\"key\"),\n",
    "        col(\"min_value\").alias(\"value\")\n",
    "    )\n",
    "\n",
    "    # DEDUP\n",
    "    df_final = df_exploded.distinct()\n",
    "\n",
    "    return df_final, new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306a2074-67b9-4759-ba59-87457784d5a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_fccg(df):\n",
    "    \"\"\"Main computation loop to find connected components, removing global variable usage.\"\"\"\n",
    "    nb_iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        nb_iteration += 1\n",
    "        \n",
    "        df, count = fccg_iterate(df)\n",
    "\n",
    "        print(f\"Number of new pairs for iteration #{nb_iteration}:\\t{count}\")\n",
    "        if count == 0:\n",
    "            print(\"\\nNo new pair, end of computation\")\n",
    "            break\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6a0c48-120e-400d-9919-3448157d1783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new pairs for iteration #1:\t7223780\nNumber of new pairs for iteration #2:\t4881437\nNumber of new pairs for iteration #3:\t3333689\nNumber of new pairs for iteration #4:\t3914963\nNumber of new pairs for iteration #5:\t1909847\nNumber of new pairs for iteration #6:\t87132\nNumber of new pairs for iteration #7:\t1326\n"
     ]
    }
   ],
   "source": [
    "final_df = compute_fccg(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37408ed9-f3a2-458b-a537-6f851b20d1d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|key|value|\n+---+-----+\n|  1|    0|\n|  6|    0|\n|  4|    0|\n| 16|    0|\n|  0|    0|\n| 14|    0|\n| 21|    0|\n| 12|    0|\n|  2|    0|\n| 11|    0|\n| 20|    0|\n| 13|    0|\n|  5|    0|\n|  8|    0|\n|  9|    0|\n|  3|    0|\n| 10|    0|\n| 19|    0|\n| 17|    0|\n| 15|    0|\n+---+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3629ebbe-034d-4edf-b154-19b0e08ce830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: 1"
     ]
    }
   ],
   "source": [
    "# Number of clusters\n",
    "final_df.select('value').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b3fe139-dadb-4f28-a3e1-aa1d4475d346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3219488038861263,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DBM-ProjectFCCG-PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
